---
title: "YouTube"
subtitle: " "
author: "Infinite <br> Alicia, Lucas, Lucia"
institute: "University of Edinburgh"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: [default, metropolis, uo-fonts]
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      
---

```{r load-packages, include = FALSE}
library(tidyverse)
library(knitr)
library(palmerpenguins)
library(tidymodels)
library(broom)
library(here)
library(dplyr)
library(stringr)
library(lubridate)
```

```{r setup, include=FALSE}
# For better figure resolution
knitr::opts_chunk$set(fig.retina = 3, dpi = 300, fig.width = 6, fig.asp = 0.618, out.width = "80%")
```


```{r load-data, include = FALSE}
USvideos <- read_csv(here("data/USvideos.csv"))
```


class: center, middle, inverse

# What makes a YouTube video popular?

---


# Introduction to USvideos

- Collected via YouTube API(Application Programming Interface)

- Consists of 4998 rows, 13 variables

- Videos collected from June 17th, 2008 to December 8th, 2017

- Videos posted within the United States

---

# Variables Used

- title, Name of the YouTube video

- category_id, Which category does the video fall under

- publish_time, What time of day did the video release 

- views, How many views did the video receive

- likes, How many likes did the video receive

- dislikes, How many dislikes did the video receive

- comment_count, How many comments did the video receive

---

class: inverse, middle, center

# Data Visualization

---

# Data Cleaning

- We had duplicate YouTube videos in separate rows so we merged the duplicate rows into one single row and also combined the views, likes and comments together. This allows for us to rank the videos with accuracy. 
```{r cleaned, echo = FALSE}
USvideos = USvideos[order(USvideos[,'title'],- USvideos[,'views']),] 
USvideos = USvideos[!duplicated(USvideos$title),]
```


---

# Most Viewed


```{r filtering, echo = FALSE}
USvideos %>% 
  group_by(title, views) %>% 
  arrange(desc(views)) %>%
  distinct(title, views)  %>%
  print(tbl_USvideos(USvideos), n = 15)
```


---

# Popular categories in the top-500-views videos

```{r common_category_ID, echo=FALSE, include = FALSE}
Common_category_ID <- USvideos %>%
  arrange(desc(views)) %>%
  slice(1:500) %>%
  count(category_id) %>%
  arrange(desc(n)) %>%
  print(tbl_USvideos(USvideos), n = 5) 
  
```


```{r pie chart, echo = FALSE}
Common_category_ID_5 <- cbind(Common_category_ID[1:5,], 
                              category = c("Music", 
                                           "Comedy", 
                                           "Entertainment",
                                           "Howto&style",
                                           "Science&Tech"))

Common_category_ID_5 <- Common_category_ID_5 %>%
  mutate(category =factor(category,
                          levels=c("Music",
                                   "Comedy",
                                   "Entertainment",
                                   "Howto&style",
                                   "Science&Tech")))

ggplot(Common_category_ID_5, aes(fill = category,
                                 x = category,
                                 y = n)) +
  geom_bar(stat = "identity") +
  labs(x = "Categories",
       y = "Frequency") +
  theme_linedraw()
```

---

# What time did the top-500-views Youtube videos upload?

```{r time-of-day, echo = FALSE, include = FALSE}

library(lubridate)

USvideos$publish_time <- hms(USvideos$publish_time)

breaks <- hour(hm("00:00", "6:00", "12:00", "18:00", "23:59"))

labels <- c("Midnight", "Morning", "Afternoon", "Evening")

USvideos$Time_of_day <- cut(x=hour(USvideos$publish_time), breaks = breaks, labels = labels, include.lowest=TRUE)

USvideos %>%
  mutate(Time_of_day)
```

```{r top-500-time-of-day, echo = FALSE}
Time_of_day_n <- USvideos %>%
  arrange(desc(views)) %>%
  slice(1:500) %>%
  count(Time_of_day) %>%
  arrange(desc(n)) %>%
  print(tbl_USvideos(USvideos), n = 5) 
```


--

 Morning: 06:00:00 ~ 11:59:59

 Afternoon: 12:00:00 ~ 17:59:59

 Evening: 18:00:00 ~ 23:59:59

 Midnight: 00:00:00 ~ 05:59:59


.footnote[
[*] The time intervals defined was recorded in Eastern Standard Time (GMT -5)
]

---

# Frequency of uploads


```{r top-500-time-of-day graph, echo = FALSE}
Time_of_day_n %>%
  ggplot(aes(x = Time_of_day, 
             y = n, 
             fill = Time_of_day)) +
  geom_bar(stat = "identity",
           position = "stack") +
  labs(x = "Time of day",
       y = "Frequency") +
  scale_fill_discrete(name = "Time of Day") +
  theme_linedraw()
```

---

<<<<<<< HEAD
# Views that each category recives, at different time
=======
# Frequency of different types of videos uploaded
>>>>>>> 4f04a487f2649c143bdb36cf56387fae6a790877

```{r time_of_day&category, echo = FALSE, include = FALSE}

Tile <- USvideos %>%
  select(category_id, views, Time_of_day) %>%
  filter(category_id%in%Common_category_ID_5$category_id) %>%
  group_by( category_id, Time_of_day) %>%
  summarise(n = sum(views)) %>%
  left_join(Common_category_ID_5[,c("category_id", "category")], by = "category_id") 
```

```{r tile plot, echo = FALSE}
options(scipen=999)
ggplot(Tile, aes(x = Time_of_day,
             y = category,
             fill = n))+
  geom_tile()+
  scale_fill_gradient(low = "white",
                      high = "red")
```

more views--darker the color
---

# Clickbait

- Clickbait: articles, photographs, etc. on the internet that are intended to attract attention and encourage people to click on links to particular websites

```{r question-or-exclamation, echo=FALSE}
click <- USvideos %>%
  filter(str_detect(title, '[:upper:]') & !str_detect(title, '[:lower:]')) %>%
  select(title, views, likes, dislikes, comment_count, ) %>%
  arrange(desc(views)) 

click[-c(1,2, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33),]

```

---


# Proportions of dislikes for all and clickbait videos


```{r dislike-proportion-comparison, echo = FALSE, include = FALSE}
options(scipen = 999)
sum_of_all_dislike <- USvideos %>%
  summarise(dislikes/views) %>%
  sum()

sum_of_all_dislike/1269

options(scipen = 999)
sum_of_click_dislike <- click %>%
  summarise(dislikes/views) %>%
  sum()

sum_of_click_dislike/30

```


$$ \frac{\sum_{n=1}^{1269} AD}{1269} = 0.0015  $$
- AD = All dislikes

$$ \frac{\sum_{n=30}^{30} CD}{30} = 0.0030  $$
- CD = Clickbait dislikes

--
 
- The proportion of dislikes for clickbait videos are twice as much of all videos!



---


# Adjusted R-squared and Root Mean Squared Error


```{r regression-line, echo = FALSE, include = FALSE}
set.seed(1203)
train_data <- training(initial_split(USvideos))
test_data <- testing(initial_split(USvideos))

corr <- linear_reg() %>%
  set_engine("lm") %>%
  fit(likes ~ views, data = train_data)

corr2 <- linear_reg() %>%
  set_engine("lm") %>%
  fit(likes ~ views+comment_count, data = train_data)
```

```{r regression, echo = FALSE, include = FALSE}
tidy(corr)
tidy(corr2)
```

```{r squared, echo = FALSE, include = FALSE}
glance(corr)$r.squared
glance(corr)$adj.r.squared

glance(corr2)$r.squared
glance(corr2)$adj.r.squared
```

```{r test-data, echo = FALSE, include = FALSE}
RMSE <- cbind(test_data, pred = predict(corr, new_data = test_data)$.pred) %>%
  summarise(sqrt(mean((likes-pred)^2)))

RMSE2 <- cbind(test_data, pred = predict(corr2, new_data = test_data)$.pred) %>%
  summarise(sqrt(mean((likes-pred)^2)))
```

**Model 1: Likes and Views**

Adjusted R squared: 0.832

Root Mean Squared Error: 64469.13

--

**Model 2:Likes and Views plus Comments**

Adjusted R squared: 0.866

RMSE2: 58060.69

When the new variable "comment_count" is added into the model, the accuracy of the prediction has no obvious improvement. Because the values of both models' adjusted R squares and Root Mean Squared Errors are almost identical.

Therefore, we implemented the first model for prediction.

---

# Least Squares Regression

```{r popular-categories, echo=FALSE, warning = FALSE, message = FALSE}
USvideos %>%
  ggplot(aes(x = views, y = likes)) +
  geom_point() +
  scale_y_continuous(limits = c(1, 500000)) +
  scale_x_continuous(limits = c(1, 25000000)) +
  geom_smooth(method = "lm", color = "Red", se = FALSE) +
  labs(x = "Number of Views",
       y = "Number of Likes") +
  theme_linedraw()
```

---


class: center, middle

# Least Squares Regression

Least Squares Regression


$$Y_i = 5212+\frac{3}{100}X_i$$

$$R^2 = 0.833 $$
  


---

# Implementation of Least Squares Regression

$$Y_i = 5212+\frac{3}{100}(2,061,202,501) $$
- The most popular video of 2017, Échame La Culpa has a total of 2,061,202,501 views as of November 28th, 2020

--

- With the least squared regression equation, we expect the total number of likes to be 61,429,046 but it is 8,900,000.



--

$$Y_i = 5212 + \frac{3}{100}(7,828,792,160) $$
- If and when the whole human population were to watch one video how many likes would it receieve?

--

- 233,303,218 likes!

--

- There will be 1 like for every 34 people. 

.footnote[
Human population: https://www.worldometers.info/world-population/

Video: https://www.youtube.com/watch?feature=youtu.be&v=TyHvyGVs42U&app=desktop
]

---

# The Most Popular Video in of 2017

```{r thumbnail, echo = FALSE, out.width = "70%", fig.align = "center", fig.cap = "Luis Fonsi, Demi Lovato - Échame La Culpa"}
knitr::include_graphics('https://maddownload.com/wp-content/uploads/2018/12/luis-fonsi-demi-lovato-echame-la-culpa-08.jpg')
```


---

class:inverse

background-image: url(https://image.freepik.com/free-vector/end-neon-style_118419-525.jpg)

