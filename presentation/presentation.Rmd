---
title: "Youtube"
subtitle: " "
author: "Infinite <br> Alicia, Lucas, Lucia"
institute: "University of Edinburgh"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: [default, metropolis, uo-fonts]
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      
---

```{r load-packages, include = FALSE}
library(tidyverse)
library(knitr)
library(palmerpenguins)
library(tidymodels)
library(broom)
library(here)
library(dplyr)
library(stringr)
library(lubridate)
```

```{r setup, include=FALSE}
# For better figure resolution
knitr::opts_chunk$set(fig.retina = 3, dpi = 300, fig.width = 6, fig.asp = 0.618, out.width = "80%")
```


```{r load-data, include = FALSE}
USvideos <- read_csv(here("data/USvideos.csv"))
```


class: center, middle, inverse

# What makes a Youtube video popular?

---


# Introduction to USvideos

- Collected via Youtube API(Application Programming Interface)

- Consists of 4998 rows, 13 variables

- Videos collected from June 17th, 2008 to December 8th, 2017

- Videos posted within the United States

---

# Variables Used

- title, Name of the Youtube video

- category_id, Which category does the video fall under

- publish_time, What time of day did the video release 

- views, How many views did the video receive

- likes, How many likes did the video receive

- dislikes, How many dislikes did the video receive

- comment_count, How many comments did the video receive

---

class: inverse, middle, center

# Data Visualization

---

# Data Cleaning

- We had duplicate Youtube videos in separate rows so we merged the duplicate rows into one single row and also combined the views, likes and comments together.

- This allows for us to rank the with accuracy because it adds up the total of the duplicate rows and gives us total values. 
```{r cleaned, echo = FALSE}
USvideos = USvideos[order(USvideos[,'title'],- USvideos[,'views']),] 
USvideos = USvideos[!duplicated(USvideos$title),]
```


---

# Most Viewed


```{r filtering, echo = FALSE}
USvideos %>% 
  group_by(title, views) %>% 
  arrange(desc(views)) %>%
  distinct(title, views)  %>%
  print(tbl_USvideos(USvideos), n = 15)
```


---

# Common categories in the top-500 videos

```{r common_category_ID, echo=FALSE, include = FALSE}
Common_category_ID <- USvideos %>%
  arrange(desc(views)) %>%
  slice(1:500) %>%
  count(category_id) %>%
  arrange(desc(n)) %>%
  print(tbl_USvideos(USvideos), n = 5) 
  
```

- Corresponding categories:

10: Music: Music videos, Albums

23: Comedy: Funny compilations, stories, stand up comedy 

24: Entertainment: Wide variety.

26: Howto & Style: Tutorials, fashion/style

28: Science & Technology: Tech reviews, innovations

---
```{r pie chart, echo = FALSE}
Common_category_ID_5 <- cbind(Common_category_ID[1:5,], 
                              category = c("Music", 
                                           "Comedy", 
                                           "Entertain",
                                           "Howto&style",
                                           "Science&Tech"))

ggplot(Common_category_ID_5, aes(fill = category,
                                 x = category,
                                 y = n))+
  geom_bar(stat = "identity")

```

---

# What time did the top 500 Youtube videos upload?

```{r time-of-day, echo = FALSE, include = FALSE}

library(lubridate)

USvideos$publish_time <- hms(USvideos$publish_time)

breaks <- hour(hm("00:00", "6:00", "12:00", "18:00", "23:59"))

labels <- c("Midnight", "Morning", "Afternoon", "Evening")

USvideos$Time_of_day <- cut(x=hour(USvideos$publish_time), breaks = breaks, labels = labels, include.lowest=TRUE)

USvideos %>%
  mutate(Time_of_day)
```

```{r top-500-time-of-day, echo = FALSE}
Time_of_day_n <- USvideos %>%
  arrange(desc(views)) %>%
  slice(1:500) %>%
  count(Time_of_day) %>%
  arrange(desc(n)) %>%
  print(tbl_USvideos(USvideos), n = 5) 
```


--

 Morning: 06:00:00 ~ 11:59:59

 Afternoon: 12:00:00 ~ 17:59:59

 Evening: 18:00:00 ~ 23:59:59

 Midnight: 00:00:00 ~ 05:59:59


.footnote[
[*] The time intervals defined was recorded in Eastern Standard Time (GMT -5)
]

---

# Frequency of uploads


```{r top-500-time-of-day graph, echo = FALSE}
Time_of_day_n %>%
  ggplot(aes(x = Time_of_day, 
             y = n, 
             fill = Time_of_day)) +
  geom_bar(stat = "identity",
           position = "stack") +
  labs(x = "Time_of_day",
       y = "Frequency") +
  scale_fill_discrete(name = "Time of Day") +
  theme_linedraw()
```

---

# Frequency of different type of videos uploaded

```{r time_of_day&category, echo = FALSE, include = FALSE}

Tile <- USvideos %>%
  select(category_id, views, Time_of_day) %>%
  filter(category_id%in%Common_category_ID_5$category_id) %>%
  group_by( category_id, Time_of_day) %>%
  summarise(n = sum(views)) %>%
  left_join(Common_category_ID_5[,c("category_id", "category")], by = "category_id") 
```

```{r tile plot, echo = FALSE}
ggplot(Tile, aes(x = Time_of_day,
             y = category,
             fill = n))+
  geom_tile()+
  scale_fill_gradient(low = "white",
                      high = "red")
```

more views--darker the color
---

# Clickbait

- Clickbait: articles, photographs, etc. on the internet that are intended to attract attention and encourage people to click on links to particular websites

```{r question-or-exclamation, echo=FALSE}
click <- USvideos %>%
  filter(str_detect(title, '[:upper:]') & !str_detect(title, '[:lower:]')) %>%
  select(title, views, likes, dislikes, comment_count, ) %>%
  arrange(desc(views)) 

click[-c(1,2, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33),]

```

---

# Clickbait

- Is clickbait real? Are clickbait videos more prone to dislikes because of a misleading title?

```{r dislike-proportion-comparison, echo = FALSE}
options(scipen = 999)
sum_of_all_dislike <- USvideos %>%
  summarise(dislikes/views) %>%
  sum()

sum_of_all_dislike/1269

options(scipen = 999)
sum_of_click_dislike <- click %>%
  summarise(dislikes/views) %>%
  sum()

sum_of_click_dislike/30

```

**Proportion of Dislikes = Dislikes/Views**

All videos' overall proportion of dislikes: 0.001504847

Clickbait videos' proportion of dislikes: 0.003008789

---
# Comparison Graph

```{r dislike-proportion-comparison-graph, echo = FALSE}
Prop <- tribble(
  ~value, ~type, ~dislikes,
  0.001504847, "All", "sum_of_all_dislikes",
  0.003008789, "Clickbait", "sum_of_clickbait_dislikes",
)
Prop %>%
  ggplot(mapping = aes(x = dislikes, y = value, fill = type)) +
  geom_col() +
  scale_y_continuous(breaks=c(0, 0.0005, 0.0010, 0.0015, 0.0020, 0.0025, 0.0030, 0.0035, 0.0040)) +
  theme_minimal() +
  labs(title = "Proportion of Dislikes",
       subtitle = "comparison between proportion of all videos and clickbait videos")
         
```

---


# Predicted Number of Views.


```{r regression-line, echo = FALSE, include = FALSE}
train_data <- training(initial_split(USvideos))
test_data <- testing(initial_split(USvideos))

corr <- linear_reg() %>%
  set_engine("lm") %>%
  fit(likes ~ views, data = train_data)

corr2 <- linear_reg() %>%
  set_engine("lm") %>%
  fit(likes ~ views+comment_count, data = train_data)
```

```{r regression, echo = FALSE, include = FALSE}
tidy(corr)
tidy(corr2)
```

```{r squared, echo = FALSE, include = FALSE}
glance(corr)$r.squared
glance(corr)$adj.r.squared

glance(corr2)$r.squared
glance(corr2)$adj.r.squared
```

```{r test-data, echo = FALSE, include = FALSE}
RMSE <- cbind(test_data, pred = predict(corr, new_data = test_data)$.pred) %>%
  summarise(sqrt(mean((likes-pred)^2)))

RMSE2 <- cbind(test_data, pred = predict(corr2, new_data = test_data)$.pred) %>%
  summarise(sqrt(mean((likes-pred)^2)))
```

model 1 adjust R square: 0.7780867

model 1 adjust R square: 0.8700796

RMSE:64676.96

RMSE2:57349.44

When new variable "comment_count" is added into the model, the accuracy of prediction is improved according to adjust R square and Root Mean Squared Error.
Therefore, we use the second model for prediction.


```{r popular-categories, echo=FALSE, warning = FALSE, message = FALSE}
USvideos %>%
  ggplot(aes(x = views, y = likes)) +
  geom_point() +
  scale_y_continuous(limits = c(1, 500000)) +
  scale_x_continuous(limits = c(1, 25000000)) +
  geom_smooth(method = "lm", color = "Red", se = FALSE) +
  labs(x = "Number of Views",
       y = "Number of Likes") +
  theme_linedraw()
```

---


class: center, middle

# Least Squares Regression

Least Squares Regression


$$Y_i = 4355+\frac{3}{100}X_i$$

$$R^2 = 0.804 $$
  


---

# Implementation of Least Squares Regression

$$Y_i = 4355+\frac{3}{100}(2,061,202,501) $$
- The most popular video of 2017, Échame La Culpa has a total of 2,061,202,501 views as of November 28th, 2020

--

- With the least squared regression equation, we expect the total number of likes to be 61,428,189 but it is 8,900,000.



--

$$Y_i = 4355 + \frac{3}{100}(7,828,792,160) $$
- If and when the whole human population were to watch one video how many likes would it receieve?

--

- 233,302,361 likes!

--

- There will be 1 like for every 34 people. 

.footnote[
Human population: https://www.worldometers.info/world-population/

Video: https://www.youtube.com/watch?feature=youtu.be&v=TyHvyGVs42U&app=desktop
]

---

# The Most Popular Video in of 2017

```{r thumbnail, echo = FALSE, out.width = "70%", fig.align = "center", fig.cap = "Luis Fonsi, Demi Lovato - Échame La Culpa"}
knitr::include_graphics('https://maddownload.com/wp-content/uploads/2018/12/luis-fonsi-demi-lovato-echame-la-culpa-08.jpg')
```


---

class:inverse

background-image: url(https://image.freepik.com/free-vector/end-neon-style_118419-525.jpg)

